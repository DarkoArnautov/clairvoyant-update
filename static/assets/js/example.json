{
  "query": {
    "count": 3,
    "created": "2018-03-22T17:27:10Z",
    "lang": "en-US",
    "results": {
      "item": [
        {
          "title": "Guide to Using Apache Kudu and Performance Comparison with HDFS.",
          "link": "https://blog.clairvoyantsoft.com/guide-to-using-apache-kudu-and-performance-comparison-with-hdfs-453c4b26554f?source=rss----df2086104f90---4",
          "guid": {
            "isPermaLink": "false",
            "content": "https://medium.com/p/453c4b26554f"
          },
          "category": [
            "apache-spark",
            "big-data",
            "apache-kudu"
          ],
          "creator": "Kruti Vanatwala",
          "pubDate": "Mon, 19 Mar 2018 16:20:22 GMT",
          "updated": "2018-03-20T19:18:25.641Z",
          "encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/345/1*-UdlA3tIizRzn2B11vG0Kg.png\" /></figure><p><a href=\"https://kudu.apache.org/\">Apache Kudu</a> is an open source columnar storage engine. It promises low latency random access and efficient execution of analytical queries. The kudu storage engine supports access via Cloudera Impala, Spark as well as Java, C++ and Python API’s.</p><p>The idea behind this article was to document my experience in exploring Apache Kudu, understanding its limitations if any and also running some experiments to compare the performance of Apache Kudu storage against HDFS storage.</p><h3>Installing Apache Kudu using Cloudera Manager</h3><p>Below is a link to the the Cloudera Manager Apache Kudu documentation and can be used to install Apache Service on a cluster managed by Cloudera Manager.</p><p><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/kudu.html\">Apache Kudu Guide | 5.14.x | Cloudera Documentation</a></p><h3>Accessing Apache Kudu via Impala</h3><p>It is possible to use Impala to CREATE, UPDATE, DELETE and INSERT into kudu stored tables. Good documentation can be found here <a href=\"https://www.cloudera.com/documentation/kudu/5-10-x/topics/kudu_impala.html\">https://www.cloudera.com/documentation/kudu/5-10-x/topics/kudu_impala.html</a>.</p><p>Creating a new table:</p><pre>CREATE TABLE new_kudu_table(id BIGINT, name STRING, PRIMARY KEY(id))<br>PARTITION BY HASH PARTITIONS 16<br>STORED AS KUDU;</pre><p>Performing insert, updates and deletes on the data:</p><pre>--insert into that table<br>INSERT INTO new_kudu_table VALUES(1, &quot;Mary&quot;);<br>INSERT INTO new_kudu_table VALUES(2, &quot;Tim&quot;);<br>INSERT INTO new_kudu_table VALUES(3, &quot;Tyna&quot;);</pre><pre>--Upsert when insert is meant to override existing row<br>UPSERT INTO new_kudu_table VALUES (3, &quot;Tina&quot;);</pre><pre>--Update a Row<br>UPDATE new_kudu_table SET name=&quot;Tina&quot; where id = 3;</pre><pre>--Update in Bulk<br>UPDATE new_kudu_table SET name=&quot;Tina&quot; where id&lt;3;</pre><pre>--Delete<br>DELETE FROM new_kudu_table WHERE id = 3;</pre><p>It is also possible to create a kudu table from existing Hive tables using CREATE TABLE DDL. In below example script if table movies already exists then Kudu backed table can be created as follows:</p><pre>CREATE TABLE movies_kudu<br>PRIMARY KEY (`movieid`)<br>PARTITION BY HASH(`movieid`) PARTITIONS 8<br>STORED AS KUDU<br>AS SELECT movieId, title, genres FROM movies;</pre><p>Limitations when creating a kudu table:</p><p><strong>Unsupported data-types:</strong> When creating a table from an existing hive table if table has VARCHAR(), DECIMAL(), DATE and complex data types(MAP, ARRAY, STRUCT, UNION) then these are not supported in kudu. Any attempt to select these columns and create a kudu table will result in an error. If a kudu table is created using <strong>SELECT *</strong>, then the incompatible non-primary key columns will be dropped in final table.</p><p><strong>Primary Key: </strong>Primary keys must be specified first in the table schema. When creating kudu table from another existing table where primary key columns are not first — reorder the columns in the select statement in the create table statement. Also Primary key columns cannot be null.</p><h3><strong>Access Kudu via Spark</strong></h3><p>Adding <a href=\"https://mvnrepository.com/artifact/org.apache.kudu/kudu-spark\">kudu_spark</a> to your spark project allows you to create a kuduContext which can be used to create kudu tables and load data to them. Note that this only creates the table within kudu and if you want to query this via Impala you would have to create an external table referencing this kudu table by name.</p><p>Below is a simple walkthrough of using kudu spark to create tables in kudu via spark. Lets start with adding the dependencies,</p><pre>&lt;properties&gt;<br>    &lt;jdk.version&gt;1.7&lt;/jdk.version&gt;<br>    &lt;spark.version&gt;1.6.0&lt;/spark.version&gt;<br>    &lt;scala.version&gt;2.10.5&lt;/scala.version&gt;<br>    &lt;kudu.version&gt;1.4.0&lt;/kudu.version&gt;</pre><pre>&lt;/properties&gt;</pre><pre>&lt;dependency&gt;<br>    &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt;<br>    &lt;artifactId&gt;kudu-spark_2.10&lt;/artifactId&gt;<br>    &lt;version&gt;${kudu.version}&lt;/version&gt;<br>&lt;/dependency&gt;</pre><p>Next create a KuduContext as shown below. As the library for SparkKudu is written in Scala, we would have to apply appropriate conversions such as converting JavaSparkContext to a Scala compatible</p><pre>import org.apache.kudu.spark.kudu.KuduContext;<br>JavaSparkContext sc = new JavaSparkContext(new SparkConf());<br>KuduContext kc = new KuduContext(&quot;&lt;master_url&gt;:7051&quot;,<br>JavaSparkContext.toSparkContext(sc));</pre><p>If we have a data frame which we wish to store to kudu we can do so as follows:</p><pre>import org.apache.kudu.client.CreateTableOptions;<br>df = … // data frame to load to kudu<br>primaryKeyList = .. //Java List of table&#39;s primary keys</pre><pre>CreateTableOptions kuduTableOptions = new CreateTableOptions();<br>kuduTableOptions.addHashPartitions( &lt;primaryKeyList&gt;, &lt;numBuckets&gt;);</pre><pre>// create a scala Seq of table&#39;s primary keys<br>Seq&lt;String&gt; primary_key_seq = JavaConversions.asScalaBuffer(primaryKeyList).toSeq();</pre><pre>//create a table with same schema as data frame<br>kc.createTable(&lt;kuduTableName&gt;, df.schema(), primary_key_seq, kuduTableOptions);</pre><pre>//load dataframe to kudu table<br>kc.insertRows(df, &lt;tableName&gt;);</pre><p>Limitations when using kudu via spark:</p><p><strong>Unsupported Datatypes: </strong>Some complex datatypes are unsupported by kudu and creating tables using them would through exceptions when loading via Spark. Spark does manage to convert the VARCHAR() to a spring type, however the other types (ARRAY, DATE, MAP, UNION and DECIMAL) would not work.</p><p><strong>We need to create External Table if we want to access via Impala: </strong>The table created in kudu using above example resides in kudu storage only and is not reflected as an Impala table. To query table via Impala we must create an external table pointing to the kudu table.</p><pre>CREATE EXTERNAL TABLE IF NOT EXISTS &lt;impala_table_name&gt; <br>STORED AS KUDU TBLPROPERTIES(&#39;kudu.table_name&#39;=&#39;&lt;kudu_table_name&gt;&#39;);</pre><h3>Apache Kudu and HDFS Performance Comparison</h3><p><strong>Objective of the Experiment</strong></p><p>The idea behind this experiment was to compare Apache Kudu and HDFS in terms of loading data and running complex Analytical queries.</p><p><strong>Experiment Setup</strong></p><ol><li><strong>Dataset Used: </strong><a href=\"http://www.tpc.org/tpch/default.asp\">The TPC Benchmark™H (TPC-H</a>) which is a decision support benchmark, emulating typical business datasets and a suite of complex analytical queries. A good source to generate this dataset and load it to hive can be found at <a href=\"https://github.com/hortonworks/hive-testbench\">https://github.com/hortonworks/hive-testbench</a>. The dataset has 8 tables and can be generated at different scales from 2 Gb onwards. For the purpose of this test 20Gb of total data is generated.</li><li><strong>Cluster Setup: </strong>The cluster has 4 Amazon EC2 instances with 1 master(m4.xlarge) and 3 data nodes(m4.large). Each cluster has 1 disk of size 150 Gb. The cluster is managed via Cloudera Manager.</li></ol><h3><strong>Data Loads Performance:</strong></h3><p>Table 1. shows time in secs between loading to kudu vs hdfs using Apache Spark. The kudu tables are hash partitioned using the primary key.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/836/1*HAmM9P04FwmipJAO1pHCrg.png\" /><figcaption>Table 1. Load times for the tables in the benchmark dataset</figcaption></figure><p>Observations: From the table above we can see that Small Kudu Tables get loaded almost as fast as Hdfs tables. However As the size increases we do see the load times becoming double that of Hdfs with the largest table Lineitem taking up-to 4 times the load time.</p><h3><strong>Analytical Queries performance:</strong></h3><p>The TPC-H Suite includes some benchmark analytical queries. The queries were run using Impala against HDFS Parquet stored table, Hdfs comma separated storage and Kudu (16 and 32 Buckets Hash Partitions on Primary Key). The runtime for each query was recorded and the charts below show a comparison of these run times in sec.</p><p><strong>Comparing Kudu with HDFS Parquet:</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hXR3BTQhCsAeQQQcDIT4AA.png\" /><figcaption>Chart 1. Running Analytical Queries on Kudu and HDFS Parquet</figcaption></figure><p>Observations: Chart 1 compares the runtimes for running benchmark queries on kudu and HDFS Parquet stored tables. We can see that the kudu stored tables do perform almost as well as HDFS Parquet stored tables, with the exception of some queries(Q4, Q13, Q18) where they take a much longer time as compared to Hdfs Parquet.</p><p><strong>Comparing Kudu with HDFS Comma Separated storage file:</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*T7v7R4TUaKB6kJ0JDdDrKg.png\" /><figcaption>Chart 2. Running Analytical Queries on Kudu and HDFS Comma Separated file</figcaption></figure><p>Observations: Chart 2 compared the kudu runtimes (same as chart 1) against HDFS Comma separated storage. Here we can see that the queries take much longer time to run on HDFS Comma separated storage as compared to Kudu, with Kudu (16 bucket storage) having runtimes on an average 5 times faster and Kudu (32 bucket storage) performing 7 times better on a avg.</p><h3><strong>Random Access Performance:</strong></h3><p>Kudu does boast of having much lower latency when randomly accessing a single row. In order to test this, I used the customer table of the same TPC-H benchmark and ran 1000 Random accesses by Id in a loop. The runtimes for these were measured for Kudu 4, 16 and 32 bucket partitioned data as well as for HDFS Parquet stored Data. The Chart below shows the runtime in sec. for 1000 Random accesses proving that Kudu indeed is the winner when it comes to random access selections.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Y5gMHx4RBYYP05EQeIEZwg.png\" /><figcaption>Chart 3. Comparing time for Random Selections</figcaption></figure><h3>Kudu Update, Insert and Delete Performance</h3><p>Since kudu does support these additional operations, this section compares the runtimes for these. The test was setup similar to the random access above with 1000 operations run in loop and runtimes measured which can be seen in Table 2 below:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_jZ6Z_JE0nfdFSZbldzDsQ.png\" /><figcaption>Table 2. Measuring Runtime for Various Operations</figcaption></figure><h3>Conclusion</h3><p>Just laying down my thoughts about Apache Kudu based on my exploration and experiments.</p><p>As far as accessibility is concerned I feel there are quite some options. It can be accessed via Impala which allows for creating kudu tables and running queries against them. SparkKudu can be used in Scala or Java to load data to Kudu or read data as Data Frame from Kudu. Additionally kudu client API’s are available in Java, Python and C++ (not covered as part of this blog).</p><p>There are some limitations with regards to datatypes supported by Kudu and if a use case requires use of complex types for columns such as Array, Map etc. then kudu would not be a good option for that.</p><p>The experiments in this blog were tests to gauge how Kudu measures up against HDFS in terms of performance.</p><p>From the tests I can see that although it does take longer to initially load data into Kudu as compared to HDFS it does give a near equal performance when it comes to running analytical queries and better performance for random access to data.</p><p>Overall I can conclude that if the requirement is for a storage which performs as well as HDFS for analytical queries with the additional flexibility of faster random access and RDBMS features such as Updates/Deletes/Inserts, then Kudu could be considered as a potential shortlist.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=453c4b26554f\" width=\"1\" height=\"1\"><hr><p><a href=\"https://blog.clairvoyantsoft.com/guide-to-using-apache-kudu-and-performance-comparison-with-hdfs-453c4b26554f\">Guide to Using Apache Kudu and Performance Comparison with HDFS.</a> was originally published in <a href=\"https://blog.clairvoyantsoft.com\">Clairvoyant Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        },
        {
          "title": "Jenkins Workspace Cleanup - Automate folders clean up for all jobs",
          "link": "https://blog.clairvoyantsoft.com/jenkins-cleanup-workspace-reduce-disk-usage-18310097d3ef?source=rss----df2086104f90---4",
          "guid": {
            "isPermaLink": "false",
            "content": "https://medium.com/p/18310097d3ef"
          },
          "category": [
            "groovy",
            "cleanup",
            "workspace",
            "jenkins",
            "build"
          ],
          "creator": "Vishnu Thummanapelli",
          "pubDate": "Tue, 09 Jan 2018 06:04:00 GMT",
          "updated": "2018-02-06T19:02:03.933Z",
          "encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*sZjUJhScS3aOITZwRQSZHw.png\" /><figcaption>© <a href=\"https://jenkins.io/blog/2017/08/23/pull-requests-and-more/\">Jenkins</a></figcaption></figure><p>If you are here, your Jenkins machine might be loaded with hundreds of jobs that are occupying a major of your disk-space with its uncleaned workspace. Especially, when your jobs are building from Maven/Gradle.</p><p>Okay, how about you use <a href=\"https://wiki.jenkins.io/display/JENKINS/Discard+Old+Build+plugin\">Discard old builds plugin</a>? You might have thought it would do wonders when you looked at the documentation. Then you realize the plugin isn’t working as expected - because it only deletes the build history and not the workspace folders?</p><p>You might have found that there is a <a href=\"https://wiki.jenkins.io/display/JENKINS/Workspace+Cleanup+Plugin\">plugin to delete your workspace</a> after executing a build. Well, it’s a straightforward thing to do. But what if you wanted to look back at the workspace after you build the job? You have nothing left in the workspace, right?</p><p>Alright, so none of these plugins solves my problem.</p><h4><strong>Problem Statement:</strong></h4><p>Persist only the latest 3 or 5 folders (sorted by date modified) in the workspace <em>or</em> by Sprint numbers(latest 3 or 5 folders based on sprint number) and remove the rest after every successful build. Also to create a nightly job to do the automatic cleanup for all jobs in Jenkins.</p><blockquote><strong>Savior: Groovy Script</strong></blockquote><h4><strong>Solution:</strong></h4><ol><li>Install <a href=\"https://wiki.jenkins.io/display/JENKINS/Groovy+Postbuild+Plugin\">Groovy Post-build plugin</a> to run Groovy script(we write the script in the next steps). <em>Why </em><strong><em>Groovy</em></strong><em>? Jenkins has built-in support for Groovy language(built on JVM and backward compatible with </em><strong><em>Java</em></strong><em>).</em></li><li>Create a new job for cleanup as a freestyle project.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5vYy9GwC_lsvGD8UW0Uxtw.png\" /><figcaption><em>Create a new job for cleanup</em></figcaption></figure><p>3. Create a parameter <strong>MAX_BUILDS</strong> as this job will be designed to be parameterized so that you have control over how many build folders you want to save.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hOHzuNTMRyzphoCkwAXuTQ.png\" /><figcaption>Create MAX_BUILDS</figcaption></figure><p>4. Create a <strong>new post-build action</strong> as <strong>Groovy Post-build </strong>and paste any of the scripts from below scripts on GitHub.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5MugpJKx0X2lA-SRHFLx2g.png\" /><figcaption>Sample screenshot. Click the below links for complete code.</figcaption></figure><h4><strong>Scenarios &amp; Scripts</strong></h4><p><strong><em>a)</em> </strong><a href=\"https://github.com/teamclairvoyant/jenkins-workspace-cleanup-groovy-script/blob/master/clean-up-one-by-date-modified.groovy\">Select folders based on Date Modified (For one job)</a></p><p><strong><em>b)</em> </strong><a href=\"https://github.com/teamclairvoyant/jenkins-workspace-cleanup-groovy-script/blob/master/clean-up-all-by-date-modified.groovy\">Select folders based on Date Modified (For all jobs)</a></p><p><strong><em>c)</em> </strong><a href=\"https://github.com/teamclairvoyant/jenkins-workspace-cleanup-groovy-script/blob/master/clean-up-one-by-sprint-numbers.groovy\">Select folders based on Sprint numbers (For one job)</a></p><p><strong><em>d)</em> </strong><a href=\"https://github.com/teamclairvoyant/jenkins-workspace-cleanup-groovy-script/blob/master/clean-up-all-by-sprint-numbers.groovy\">Select folders based on Sprint numbers (For all jobs)</a></p><p>The code is self-explanatory with comments.</p><p>5. Now you can configure an existing job in Jenkins to use the Cleanup job you just created. Open Configuration of an existing job and add a post-build action to trigger the above created new clean-up job. So, after Job A executes and builds, Job B (clean up its own workspace) will run automatically.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*soRytgDiWeknbWHAE2Qw0Q.png\" /><figcaption>Post-build action to trigger</figcaption></figure><p>(<strong>Or</strong>)</p><p>You can automate this job for all the jobs in Jenkins as a nightly job. To do this, create a build-trigger to run at a certain time every week.</p><blockquote>Note: (Only configure this for either of <strong>b</strong> <em>or</em><strong><em> </em>d<em> </em></strong><em>scenarios</em><strong><em> </em></strong>mentioned above).</blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qHa_7eK-Ge_kUijNWcohyg.png\" /><figcaption>Scheduled cleanup every Sunday midnight.</figcaption></figure><p>To check what folders are deleted and saved, please check the Console output of builds in the Clean up job.</p><p>Please post in comments if you have any errors or questions. Will be happy to help. This guide helps as a starter to get into internal filesystem of Jenkins through Groovy. For more exploration, you can try the<a href=\"https://wiki.jenkins.io/display/JENKINS/Jenkins+Script+Console\"> Jenkins script console</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=18310097d3ef\" width=\"1\" height=\"1\"><hr><p><a href=\"https://blog.clairvoyantsoft.com/jenkins-cleanup-workspace-reduce-disk-usage-18310097d3ef\">Jenkins Workspace Cleanup - Automate folders clean up for all jobs</a> was originally published in <a href=\"https://blog.clairvoyantsoft.com\">Clairvoyant Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        },
        {
          "title": "The Data Anonymization Solution",
          "link": "https://blog.clairvoyantsoft.com/the-data-anonymization-solution-bef47f023d78?source=rss----df2086104f90---4",
          "guid": {
            "isPermaLink": "false",
            "content": "https://medium.com/p/bef47f023d78"
          },
          "category": [
            "news",
            "privacy"
          ],
          "creator": "vikram bhalchandra",
          "pubDate": "Tue, 06 Feb 2018 15:36:57 GMT",
          "updated": "2018-02-06T15:36:57.531Z",
          "encoded": "<p><strong>January 25, 2018</strong></p><p><strong>By Chandra Ambadipudi</strong></p><p><a href=\"https://ababankmarketing.com/insights/data-anonymization-solution/\">https://ababankmarketing.com/insights/data-anonymization-solution/</a></p><p><strong>How banks can solve their marketing and compliance headaches in one fell swoop.</strong></p><p>We’ve all at one time or another gotten an alert from our bank when it thought fraud had occurred. But what about a heads up when someone is overcharging us?</p><p>Since last year, Capital One’s <a href=\"http://www.tearsheet.co/modern-banking-experience/how-banks-are-using-customer-data-for-personalized-experiences?utm_source=digiday.com&amp;utm_medium=referral&amp;utm_campaign=digidaydis&amp;utm_content=bhattacharyya-how-banks-are-using-customer-data-for-personalized-experiences\">Second Look program </a>predicpredichas alerted customers of suspicious activity like being charged twice for the same expense. Leveraging data for customer outreach like that can help reduce churn and build brand affinity. When applied to advertising and marketing, such data can make spending more effective and help marketers create more relevant, personalized messages that drive higher customer engagement.</p><p>Most banks struggle to harness their data like that.</p><p>One reason is that data is <a href=\"https://ababankmarketing.com/insights/data-analytics-drive-positive-change/\">often siloed across their network </a>so it’s hard to put into action. Another is that the data is of a sensitive nature so banks are bound by ethics and government regulations (e.g. PCI, NCUA, GDPR) that can create challenges for putting it to use.</p><p>But there’s a way to both ensure compliance and leverage the data — albeit in an anonymized fashion — for use in marketing.</p><p><strong>Data for banks is a double-edged sword.</strong></p><p>In 2018, all marketers use data — or at least pay lip service to the idea. Like many other sectors, such as retail, financial services firms have access to lots of high-quality, actionable data. For instance, a bank may see that a customer has taken a lot of trips to Home Depot lately and <a href=\"https://www.wsj.com/articles/banks-get-personal-in-their-marketing-1493086141\">conclude</a> that they’re up for a pitch for a home equity line of credit.</p><p>In the words of Voltaire and Spider-man though, with great power comes great responsibility.</p><p>Access to huge amounts of personal data increases the risk that some of it will run afoul of compliance standards — especially in highly regulated industries like financial services and healthcare/pharma. The looming implementation of the EU’s General Data Protection Regulation (GDPR) in May has <a href=\"https://www.americanbanker.com/news/eus-new-data-privacy-law-creates-headaches-for-us-banks\">upped the ante</a>. Use of data that hasn’t been procured with an EU resident’s consent can result in fines of up to 4% of revenues. For countries like France and Germany, such stringent standards are nothing new.</p><p><strong>How anonymization meets both needs.</strong></p><p>Regulations ranging from the <a href=\"http://s.bl-1.com/h/XxzS8tS?url=https://www.ftc.gov/tips-advice/business-center/privacy-and-security/gramm-leach-bliley-act\">Gramm-Leach-Bliley Act (GLBA)</a> to GDPR compel banks to audit their data on a quarterly basis and weed out data that is held without their customers’ consent.</p><p>Banks don’t need to dump that data wholesale though. Anonymizing such data can allow banks to still use it for marketing purposes with broad aspects and attributes on which they can do larger customized campaigns even if not at an individual level — and still stay on the right side of the privacy issue.</p><p>In a kill-two-birds-with-one-stone solution, banks can go about segmenting such anonymized data while they cleanse it. Better yet, they can automate the process and execute it on a regular basis to make such data available in real-time. Many systems designed for compliance continuously monitor data sources to ensure compliance. That same real-time monitoring can also mask data and then glean insights to apply to marketing.</p><p>Banks, of course, handle a huge amount of data, but that’s not unusual these days. Marketers focused around online marketing and customer data handle volumes that have grown to billions of transactions per day. These vast volumes of data have made it hard for banks (and other organizations) to understand what sensitive info exists in their ecosystem and where it exists.</p><p>In this case, the needs of marketing and compliance have dovetailed to make way for a solution that meets both needs. For customers of banks and credit unions, that means that they can trust their financial institutions to handle their data responsibly and to use it intelligently.</p><p><em>Chandra Ambadipudi is the CEO of </em><a href=\"http://clairvoyantsoft.com/\"><em>Clairvoyant</em></a><em>, a leading big data consulting company, and </em><a href=\"http://kogni.io/\"><em>Kogni</em></a><em>, a platform for companies to discover and secure sensitive data in enterprise data sources.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bef47f023d78\" width=\"1\" height=\"1\"><hr><p><a href=\"https://blog.clairvoyantsoft.com/the-data-anonymization-solution-bef47f023d78\">The Data Anonymization Solution</a> was originally published in <a href=\"https://blog.clairvoyantsoft.com\">Clairvoyant Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        }
      ]
    }
  }
}